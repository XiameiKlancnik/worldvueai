# WorldVue New Pipeline Guide

**LLM Judges + Transformers, Hybrid Pairing, Budget Controls**

This guide explains how to use the new WorldVue pipeline that replaces weak labeling with LLM judges and transformer cross-encoders.

## üÜï OPTIMIZED PIPELINE (Recommended)

**New Features:**
- ‚úÖ **No Ties**: Eliminates 56% tie rate that caused 30-40% accuracy
- ‚úÖ **GPT-4o-mini**: 10x cheaper than GPT-3.5-turbo
- ‚úÖ **Balanced Data**: Automatic 50/50 A/B splits
- ‚úÖ **Timestamped Artifacts**: Preserves all previous runs
- ‚úÖ **90% Cost Reduction**: Multi-axis judging + optimized prompts

### Run Optimized Pipeline (One Command)

```bash
# Automatic timestamped run - won't overwrite existing artifacts
python run_optimized_pipeline.py
```

This creates `artifacts_run_YYYYMMDD_HHMMSS_optimized_no_ties/` with all outputs.

### Cost Comparison

| Feature | Original Pipeline | Optimized Pipeline | Savings |
|---------|------------------|-------------------|---------|
| **Model** | GPT-3.5-turbo | GPT-4o-mini | 10x cheaper |
| **Ties** | 56% ties ‚Üí broken training | 0% ties ‚Üí 70-85% accuracy | Quality fix |
| **API calls** | 5 calls per pair | 1 call per pair | 5x fewer |
| **Total cost** | $50-100 per 1000 pairs | $2-5 per 1000 pairs | **90%+ savings** |
| **Training accuracy** | 30-40% | 70-85% | **2x improvement** |

### Manual Steps (If Needed)

```powershell
# PowerShell (Windows) - Use direct folder names:
$TIMESTAMP = Get-Date -Format "yyyyMMdd_HHmmss"
$RUN_DIR = "artifacts_run_${TIMESTAMP}_manual"
mkdir $RUN_DIR

# Run steps with new artifacts folder
worldvue clusters make --articles articles_with_embeddings.parquet --budget worldvue/configs/budget.yaml --out $RUN_DIR/clusters.parquet
worldvue pairs make --clusters $RUN_DIR/clusters.parquet --budget worldvue/configs/budget.yaml --out $RUN_DIR/pairs.parquet
worldvue judge style --pairs $RUN_DIR/pairs.parquet --articles articles_with_embeddings.parquet --budget worldvue/configs/budget.yaml --out $RUN_DIR/judge_results_no_ties.jsonl
worldvue pairs labels --in $RUN_DIR/judge_results_no_ties.jsonl --out $RUN_DIR/pairs_labeled_balanced.parquet --pairs $RUN_DIR/pairs.parquet
worldvue train style --pairs-labeled $RUN_DIR/pairs_labeled_balanced.parquet --articles articles_with_embeddings.parquet --out-dir $RUN_DIR/models/ --epochs 3
```

```bash
# Command Prompt (Windows) - Even simpler:
mkdir artifacts_run_manual_20250927
worldvue clusters make --articles articles_with_embeddings.parquet --budget worldvue/configs/budget.yaml --out artifacts_run_manual_20250927/clusters.parquet
worldvue pairs make --clusters artifacts_run_manual_20250927/clusters.parquet --budget worldvue/configs/budget.yaml --out artifacts_run_manual_20250927/pairs.parquet
```

```bash
# Linux/Mac:
RUN_DIR=artifacts_run_$(date +%Y%m%d_%H%M%S)_manual
mkdir $RUN_DIR
# Then use $RUN_DIR in commands above
```

## Quick Start (Preview Mode)

Test the entire pipeline with a small budget:

```bash
# Run preview mode (600 articles, 1000 pairs, dry_run=true)
# For Windows PowerShell (single line):
worldvue run preview --articles articles_with_embeddings.parquet --budget worldvue/configs/budget_preview.yaml --workspace /tmp/worldvue_test

# For Linux/Mac (with line continuations):
worldvue run preview \
  --articles articles_with_embeddings.parquet \
  --budget worldvue/configs/budget_preview.yaml \
  --workspace /tmp/worldvue_test
```

This runs the complete pipeline end-to-end with mock LLM calls for testing.

## Budget Planning

Always start by planning your budget:

```bash
# Estimate costs before running
# For Windows PowerShell (single line):
worldvue budget plan --articles articles_with_embeddings.parquet --budget worldvue/configs/budget.yaml --out artifacts/budget_report.md

# For Linux/Mac (with line continuations):
worldvue budget plan \
  --articles articles_with_embeddings.parquet \
  --budget worldvue/configs/budget.yaml \
  --out artifacts/budget_report.md
```

## Data Preparation

### 0. Prepare Your Data

Before running the pipeline, you need to prepare your articles:

```bash
# If you have CSV data, convert to Parquet:
python -c "import pandas as pd; df = pd.read_csv('all_articles.csv'); df.to_parquet('all_articles.parquet')"

# Generate embeddings (required!):
worldvue embed --input all_articles.parquet

# Add required columns:
python -c "
import pandas as pd
import joblib

# Load articles and embeddings
df = pd.read_parquet('all_articles.parquet')
cache = joblib.load('artifacts/article_embeddings.joblib')

# Add embeddings to dataframe
embeddings = [cache.get(row['id']) for _, row in df.iterrows()]
df['embedding'] = embeddings

# Add required columns
df['article_id'] = df['id']
df['country'] = df['source_country']

# Save
df.to_parquet('articles_with_embeddings.parquet')
print(f'Prepared {len(df)} articles with embeddings')
"
```

---

## ‚ö†Ô∏è LEGACY PIPELINE (Not Recommended)

**WARNING**: The commands below use the old `artifacts/` folder and will **OVERWRITE** your existing results. Use the **OPTIMIZED PIPELINE** above instead for:
- Timestamped artifacts (no overwrites)
- GPT-4o-mini (90% cost savings)
- No ties (fixes 30-40% accuracy issue)
- Balanced training data

---

## Full Pipeline Steps (Legacy - Use Optimized Above Instead)

### 1. Budget Configuration

Edit `worldvue/configs/budget.yaml`:

```yaml
# Article sampling
max_articles_global: 1500
max_articles_per_country: 60
cluster_min_size: 6

# Pair sampling
target_pairs_total: 1000
cross_country_ratio: 0.40

# LLM judging
votes_per_pair: 3
dry_run: false  # Set to true for testing
price_per_mtoken_usd: 0.15
use_multi_axis_judging: true  # 5x faster - judge all axes in one call

# Scoring
scoring_method: "anchors"  # or "bt_offsets"
country_isotonic_calibration: true
```

### 2. Global Clustering

Create topic clusters across all countries:

```bash
# For Windows PowerShell (single line):
worldvue clusters make --articles articles_with_embeddings.parquet --budget worldvue/configs/budget.yaml --out artifacts/clusters.parquet

# For Linux/Mac (with line continuations):
worldvue clusters make \
  --articles articles_with_embeddings.parquet \
  --budget worldvue/configs/budget.yaml \
  --out artifacts/clusters.parquet
```

### 3. Hybrid Pair Sampling

Sample within-country and cross-country pairs:

```bash
# For Windows PowerShell (single line):
worldvue pairs make --clusters artifacts/clusters.parquet --budget worldvue/configs/budget.yaml --out artifacts/pairs.parquet

# For Linux/Mac (with line continuations):
worldvue pairs make \
  --clusters artifacts/clusters.parquet \
  --budget worldvue/configs/budget.yaml \
  --out artifacts/pairs.parquet
```

### 4. LLM Judging

Judge pairs on five style axes:

```bash
# For Windows PowerShell (single line):
worldvue judge style --pairs artifacts/pairs.parquet --articles articles_with_embeddings.parquet --budget worldvue/configs/budget.yaml --out artifacts/judge_results.jsonl

# For Linux/Mac (with line continuations):
worldvue judge style \
  --pairs artifacts/pairs.parquet \
  --articles articles_with_embeddings.parquet \
  --budget worldvue/configs/budget.yaml \
  --out artifacts/judge_results.jsonl
```

### 5. Label Processing

Convert judge results to training labels:

```bash
# For Windows PowerShell (single line):
worldvue pairs labels --in artifacts/judge_results.jsonl --out artifacts/pairs_labeled.parquet

# For Linux/Mac (with line continuations):
worldvue pairs labels \
  --in artifacts/judge_results.jsonl \
  --out artifacts/pairs_labeled.parquet
```

### 6. Cross-Encoder Training

Train transformer models for each axis:

```bash
# For Windows PowerShell (single line):
worldvue train style --pairs-labeled artifacts/pairs_labeled.parquet --articles articles_with_embeddings.parquet --out-dir artifacts/models/ --epochs 3

# For Linux/Mac (with line continuations):
worldvue train style \
  --pairs-labeled artifacts/pairs_labeled.parquet \
  --articles articles_with_embeddings.parquet \
  --out-dir artifacts/models/ \
  --epochs 3
```

### 7. Article Scoring

Score all articles using trained models:

```bash
# For Windows PowerShell (single line):
worldvue score style --articles articles_with_embeddings.parquet --models artifacts/models/ --budget worldvue/configs/budget.yaml --clusters artifacts/clusters.parquet --out artifacts/style_scores.parquet

# For Linux/Mac (with line continuations):
worldvue score style \
  --articles articles_with_embeddings.parquet \
  --models artifacts/models/ \
  --budget worldvue/configs/budget.yaml \
  --clusters artifacts/clusters.parquet \
  --out artifacts/style_scores.parquet
```

### 8. Evaluation

Validate the scoring system:

```bash
# For Windows PowerShell (single line):
worldvue eval style --models-dir artifacts/models/ --test-pairs artifacts/pairs.parquet --scores artifacts/style_scores.parquet --articles articles_with_embeddings.parquet --budget worldvue/configs/budget.yaml --out artifacts/evaluation.json

# For Linux/Mac (with line continuations):
worldvue eval style \
  --models-dir artifacts/models/ \
  --test-pairs artifacts/pairs.parquet \
  --scores artifacts/style_scores.parquet \
  --articles articles_with_embeddings.parquet \
  --budget worldvue/configs/budget.yaml \
  --out artifacts/evaluation.json
```

## Style Axes

The system evaluates articles on five style dimensions:

1. **One-Sidedness**: Multiple perspectives vs single viewpoint
2. **Hype**: Measured tone vs sensationalized language
3. **Sourcing**: Quality and transparency of sources
4. **Fight vs Fix**: Focus on conflict vs solutions
5. **Certain vs Caution**: Absolute claims vs acknowledging uncertainty

## Budget Controls

All commands respect hard budget limits:

- **Article caps**: Global and per-country limits
- **Pair limits**: Total pairs and per-article degree constraints
- **Cost controls**: Estimated LLM costs with confirmation prompts
- **Dry run mode**: Test pipeline without LLM calls

## Key Features

- **Global Clustering**: Cross-country topic clusters for comparison
- **Hybrid Pairing**: 60% within-country, 40% cross-country pairs
- **LLM Judges**: Multi-vote pairwise style comparisons
- **Cross-Encoders**: Transformer models trained on LLM judgments
- **Anchor Scoring**: Reference texts for global score calibration
- **Country Calibration**: Isotonic regression for cross-country fairness
- **Comprehensive Evaluation**: Validates global comparability

## Troubleshooting

### Common Issues

1. **Missing embeddings**: Run `worldvue embed` first
2. **Missing columns**: Ensure your data has `article_id`, `country`, and `embedding` columns
3. **Unicode errors on Windows**: Fixed by replacing arrow character in source code
4. **PowerShell syntax errors**: Use single-line commands without backslashes
5. **File not found**: Use `articles_with_embeddings.parquet` not `data/articles.parquet`
6. **Budget exceeded**: Adjust limits in config or use preview mode
7. **Training failures**: Check data quality and reduce batch size
8. **Evaluation errors**: Ensure test data has required columns

### Debug Mode

Use preview mode with dry_run=true to test without costs:

```bash
# For Windows PowerShell (single line):
worldvue run preview --budget worldvue/configs/budget_preview.yaml --articles articles_with_embeddings.parquet

# For Linux/Mac (with line continuations):
worldvue run preview \
  --budget worldvue/configs/budget_preview.yaml \
  --articles articles_with_embeddings.parquet
```

### Windows PowerShell Tips

1. **Always use single-line commands** - PowerShell doesn't handle line continuations well
2. **Check file paths** - Use actual filenames, not example paths from docs
3. **Set encoding if needed**: `set PYTHONIOENCODING=utf-8` before running commands

## Migration from Weak Labels

The old weak labeling system has been replaced. Key changes:

- ‚ùå `worldvue weak` commands removed
- ‚úÖ `worldvue judge style` for LLM-based labeling
- ‚úÖ `worldvue train style` for cross-encoder training
- ‚úÖ Budget controls throughout pipeline
- ‚úÖ Global clustering and cross-country pairing

## Performance

Expected performance with default settings:

- **Pairwise Accuracy**: >70% average across axes
- **Anchor Consistency**: >95% in major languages
- **Country Calibration**: 80%+ countries within 50¬±3 range
- **Processing Speed**: ~1000 pairs/hour (with LLM calls)

For questions or issues, check the evaluation report after running the pipeline.